{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RAINBENCH_DATASET_EXPLORE",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrZombie69232/Simplilearn_Projects/blob/main/RAINBENCH_DATASET_EXPLORE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uis4Vp1cEXKD"
      },
      "source": [
        "![spaceml](https://drive.google.com/uc?id=1fwi6BLJhnMv0vkWfdKbscgHvtPNEparz)\n",
        "# RainBench Notebook \n",
        "In this notebook, we will demonstrate how to download and explore a small sample (1GB) of the main RainBench dataset.<br>\n",
        "Our full data loaders and machine learning pipelines can be found at our public code repository (https://github.com/FrontierDevelopmentLab/PyRain.git). <br>\n",
        "Results from the analysis of the main dataset is published at AAAI (https://arxiv.org/abs/2012.09670):<br>\n",
        "\n",
        "##The Digital Twin Team<br>\n",
        "Researchers:<br>\n",
        "\n",
        "*Christian Schroeder de Witt - University of Oxford*<br>\n",
        "*Catherine Tong - University of Oxford*<br>\n",
        "*Valentina Zantedeschi - Inria, Lille and University College London*<br>\n",
        "*Daniele De Martini - University of Oxford*<br>\n",
        "\n",
        "Faculty:<br>\n",
        "\n",
        "*Freddie Kalaitzi - University of Oxford*<br>\n",
        "*Matthew Chantry - University of Oxford*<br>\n",
        "*Duncan Watson-Parris - University of Oxford*<br>\n",
        "*Piotr Bilinski - University of Oxford and University of Warsaw*<br>\n",
        "\n",
        "**This notebook will let you explore the RainBench Dataset**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OWuyWH3Ru8v"
      },
      "source": [
        "# Getting set up\n",
        "In this section we will import required packages, get you authenticated and set up with the INARA public data bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi1lg_x0FrgX"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import datetime\n",
        "from google.cloud import storage\n",
        "from google.colab import auth\n",
        "import torch\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqH6zNG0SHcI"
      },
      "source": [
        "**The cell below will ask you to authenticate with a Google Cloud**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajhGe5sYGAQN"
      },
      "source": [
        "#Authenticate user\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "280zRWBmU6JQ"
      },
      "source": [
        "\n",
        "\n",
        "**Now we will connect to a GCP bucket and have a look at what's inside**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ4TS997Cd7l"
      },
      "source": [
        "bucket_name = 'aaai_release'\n",
        "client = storage.Client(project='fdl-europe-dte')\n",
        "gcs_bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "print('Current files in bucket:')\n",
        "# List all the available files in bucket\n",
        "file_list = list(gcs_bucket.list_blobs())\n",
        "#print all files in the bucket as blobs\n",
        "for f in file_list:\n",
        "  print(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHOP4ygcmPVh"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCqdJEos76Tx"
      },
      "source": [
        "**We will only download and explore the data from the samples folder**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7qMK4wMXIU_"
      },
      "source": [
        "# create directories to save data\n",
        "!mkdir samples/\n",
        "!mkdir samples/era5625_sample/\n",
        "!mkdir samples/imerg5625_sample/\n",
        "!mkdir samples/simsat5625_sample/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nbtsEzVjgRP"
      },
      "source": [
        "# only download the files from samples folder (this takes about a minute)\n",
        "dir = 'samples'\n",
        "file_list = list(gcs_bucket.list_blobs(prefix=dir))\n",
        "for blob in file_list:\n",
        "  blob.download_to_filename(blob.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXiXIJ0hm1Ba"
      },
      "source": [
        "datapath = [\"/content/samples/era5625_sample/era5625_sample.dill\", \n",
        "            \"/content/samples/imerg5625_sample/imerg5625_sample.dill\", \n",
        "            \"/content/samples/simsat5625_sample/simsat5625_sample.dill\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UiFWYhCSgOG"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NowclsW8cZb"
      },
      "source": [
        "**We will now use the Dataset object from our code repository to parse the sampled data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfWsR2SY86jq"
      },
      "source": [
        "**We will need to supply the Datset with the following:**\n",
        "\n",
        "\n",
        "1.   Dataset paths: specified by the *.dill* files\n",
        "2.   Partition Configuration: how to partition the dataset temporally\n",
        "3.   Sampling Configuration: what variables to sample when\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjw5WPN39vSg"
      },
      "source": [
        "rootdir = os.getcwd()\n",
        "datapath = [\"samples/era5625_sample/era5625_sample.dill\", \n",
        "            \"samples/imerg5625_sample/imerg5625_sample.dill\", \n",
        "            \"samples/simsat5625_sample/simsat5625_sample.dill\"]\n",
        "datapath = [rootdir + '/' + p for p in datapath]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltewcrpMb3-p"
      },
      "source": [
        "# write partition configuration\n",
        "\n",
        "# We specify the time ranges for train and test set respectively. \n",
        "\n",
        "partition_conf = {\"train\":\n",
        "    {\"timerange\": (\n",
        "        datetime.datetime(2018, 1, 1, 0).timestamp(), datetime.datetime(2018, 12, 31, 0).timestamp()),\n",
        "        \"increment_s\": 60 * 60},\n",
        "    \"test\":\n",
        "        {\"timerange\": (datetime.datetime(2019, 1, 15, 0).timestamp(),\n",
        "                       datetime.datetime(2019, 12, 31, 23).timestamp()),\n",
        "         \"increment_s\": 60 * 60}}\n",
        "partition_type = \"range\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYp_GGFCM999"
      },
      "source": [
        "# write sampling configurations\n",
        "\n",
        "# \"sample\" can be interpreted as the input x to a Machine learning model, and \"label\" as its output y.\n",
        "# Thus, under \"sample\", we specify the input variables. \n",
        "# Given as example are land-sea mask (lsm), latitude (lat), 2-metre temperature (t2m), surface pressure (sp).\n",
        "# t2m and sp are temporal variables which we would like to sample from -3 to 0 hours from the event horizon.\n",
        "# Next, under \"label\", we specify the output variable, precipitation (tp).\n",
        "# We would like to sample this under 4 modes: +1 to +9 hours from the event horizon.\n",
        "\n",
        "sample_conf = {\"lead_time_{}\".format(int(lt / 3600)):  # sample modes\n",
        "    {\n",
        "        \"sample\":  # sample sections\n",
        "            {\n",
        "                \"lsm\": {\"vbl\": \"era5625/lsm\"},  # sample variables\n",
        "                \"lat\": {\"vbl\": \"era5625/lat2d\"},\n",
        "                \"t2m\": {\"vbl\": \"era5625/t2m\",\n",
        "                             \"t\": np.array([0, -1, -2, -3, ]) * 3600,\n",
        "                             \"interpolate\": [\"nan\", \"nearest_past\", \"nearest_future\"][1]},\n",
        "                \"sp\": {\"vbl\": \"era5625/sp\",\n",
        "                             \"t\": np.array([0, -1, -2, -3, ]) * 3600,\n",
        "                             \"interpolate\": [\"nan\", \"nearest_past\", \"nearest_future\"][1]},\n",
        "            },\n",
        "        \"label\":\n",
        "            {\n",
        "                \"tp\": {\"vbl\": \"era5625/tp\",\n",
        "                       \"t\": np.array([lt]),\n",
        "                       \"interpolate\": [\"nan\", \"nearest_past\", \"nearest_future\"][1]}}\n",
        "    }\n",
        "    for lt in np.array([1, 3, 6, 9]) * 3600}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2rH0fkx-c2l"
      },
      "source": [
        "**We import the Dataset object from our repository**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRHeI-tB95O9"
      },
      "source": [
        "!git clone https://github.com/FrontierDevelopmentLab/PyRain.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjcDahi7-kTD"
      },
      "source": [
        "cd PyRain/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq2bQl5u-iwL"
      },
      "source": [
        "from src.dataloader import Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6u8XmFNSzug"
      },
      "source": [
        "# load dataset using defined configurations\n",
        "dataset = Dataset(datapath=datapath,\n",
        "                  partition_conf=partition_conf,\n",
        "                  partition_type=partition_type,\n",
        "                  partition_selected=\"train\",\n",
        "                  sample_conf=sample_conf,\n",
        "                  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZVLvG_QBTg8"
      },
      "source": [
        "# One may inspect the corresponding .json file, which details the precise variable name contained in the memmap files (e.g. era5625/lat2d).\n",
        "with open('/content/samples/era5625_sample/era5625_sample_info.json', 'rb') as f:\n",
        "  json_file = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvYa1q_k_CWN"
      },
      "source": [
        "\n",
        "# Exploring the Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7IrP6PrB93z"
      },
      "source": [
        "**One can now use a dataloader object to read batches of data from the dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBfdaInDCRF0"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "loader = DataLoader(dataset, batch_size=2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGf7Qde5CXGE"
      },
      "source": [
        "We can inspect an example batch and find that each datapoint contains the following:\n",
        "\n",
        "\n",
        "\n",
        "1.  samples\n",
        "2.  labels\n",
        "3.   sample modes\n",
        "4.  sample timestamps\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBeieqQMCVCa"
      },
      "source": [
        "batch = next(iter(loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_ih07eTCZmu",
        "outputId": "e174364c-31fb-40e5-b088-cf77c8afd9bc"
      },
      "source": [
        "print(batch[0].keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['sample', 'label', '__sample_modes__', '__sample_ts__'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs_hh0E_Cwlj",
        "outputId": "96848bd3-542a-48f9-9adf-21fec25a669d"
      },
      "source": [
        "# We can inspect a random datapoint from the batch.\n",
        "\n",
        "idx = 1\n",
        "\n",
        "print('Sample Mode:', batch[0]['__sample_modes__'][idx])\n",
        "print('\\nSample timestamps:')\n",
        "\n",
        "for ts in batch[0]['sample']['t2m__ts'][idx]:\n",
        "  print(datetime.datetime.fromtimestamp(ts))\n",
        "\n",
        "print('\\nLast time in sample:')\n",
        "print(datetime.datetime.fromtimestamp(batch[0]['__sample_ts__'][0][idx]))\n",
        "\n",
        "print('\\nLabel timestamps:')\n",
        "print(datetime.datetime.fromtimestamp(batch[0]['label']['tp__ts'][idx,0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Mode: lead_time_3\n",
            "\n",
            "Sample timestamps:\n",
            "2018-03-04 13:00:00\n",
            "2018-03-04 12:00:00\n",
            "2018-03-04 11:00:00\n",
            "2018-03-04 10:00:00\n",
            "\n",
            "Last time in sample:\n",
            "2018-03-04 13:00:00\n",
            "\n",
            "Label timestamps:\n",
            "2018-03-04 16:00:00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwgxOVyGC3Bc"
      },
      "source": [
        "We can also inspect the shapes of the data returned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mkm4lTKC1q4",
        "outputId": "332d8515-9e94-4be1-d8f7-d81a8dbd0589"
      },
      "source": [
        "print('lat: ', batch[0]['sample']['lat'].shape)\n",
        "print('t2m: ', batch[0]['sample']['t2m'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lat:  torch.Size([2, 1, 32, 64])\n",
            "t2m:  torch.Size([2, 4, 1, 32, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWBlMIzvDBOz"
      },
      "source": [
        "The dataset also supports parsing by timestamps and data categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCV3CzIGXx6j"
      },
      "source": [
        "tp = dataset[((datetime.datetime(2018,1,1,0).timestamp(), datetime.datetime(2018,12,31,0).timestamp(), 3600), [\"era5625/tp\"], None)]\n",
        "imerg = dataset[((datetime.datetime(2018,1,1,0).timestamp(), datetime.datetime(2018,12,31,0).timestamp(), 3600), [\"imerg5625/precipitationcal\"], None)]\n",
        "simsat = dataset[((datetime.datetime(2018,1,1,0).timestamp(), datetime.datetime(2018,12,31,0).timestamp(), 3*3600), [\"simsat5625/clbt:0\"], {\"interpolate\":\"nearest_past\"})]\n",
        "simsat2 = dataset[([datetime.datetime(2018,1,1,0).timestamp(), datetime.datetime(2018,12,31,0).timestamp()], [\"simsat5625/clbt:0\"], {})]\n",
        "tp = dataset[((datetime.datetime(2018,1,1,0).timestamp(), datetime.datetime(2018,12,31,0).timestamp(), 3600), [\"era5625/sp\"], None)]\n",
        "tp = dataset[((datetime.datetime(2018,1,1,0).timestamp(), datetime.datetime(2018,12,31,0).timestamp(), 3600), [\"era5625/t2m\"], None)]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}